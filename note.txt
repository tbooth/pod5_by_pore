I'm currently processing 28161SD which needs duplex re-calling. It's taking a long time.
Not only is this slow for the customer, but there is a risk that a job fails and many hours
of precious GPU time are lost.

We cannot process one POD5 file at a time because there may be duplex pairs split across
POD5 files. But if we were to re-assort the POD5 files by channel than this would be fine.

See the discussion at:

https://github.com/nanoporetech/dorado/issues/264

Actually, re-assorting the POD5 files by channel may be something that Hesiod should do
as a matter of course. It would be useful for us and for our customers. It would make the
duplex basecalling quite east to run in parallel. So, how to do that efficiently?

First I'd need to scan the POD5 files to extract a "read_id,channel" list. I need to
do this up-front, or else with a checkpoint rule, because I don't want to create empty
POD5 files for unused channels.

So:

rule get_channels_txt:
    output: "{foo}.pod5.channels"
    input:  "{foo}.pod5"
    shell:
        "pod5 view -H {input} -i read_id,channel > {output}"

rule get_channels_in_use:
    output: "channels_in_use.txt"
    input:  expand("{foo}.pod5.channels", ...)
    shell:
        "cut -d, -f 1 {input} | sort -u > {output}"

Now I make a POD5 file for every line in "channels_in_use.txt". A naive way would be to run "pod5 subset"
on the whole input directory with all the .channels files. But apparently this is super slow.

Another naive way would be to run "pod5 filter" on every pod5 file, for every channel (or the same with
"pod5 subset") but that would create a huge profusion of temporary files.

I think what I need is to make a job per "channels_in_use":

rule get_pod5_for_channel:
    output: "channel_{c}.pod5"
    input:  i_pod5_for_channel
    shell:
        ???

Here, the i_pod5_for_channel function wants to read through all the .channels files to just get the
relevant input pod5 files. So maybe to make this more efficient, the get_channels_in_use rule should
make a mapping of channel -> pod5_files (in JSON) which would immediately yield this info.
It makes for a more complex chackpoint scenario, as we're waiting on the checkpoint rule to see
what POD5 files should be created and also what the inputs are for each one. But I believe this
should be fine in Snakemake?!

Anyway, now the get_pod5_for_channel rule should run "pod5 filter" on just the relevant POD5 files
to make a final output file. Not sure if I can do this in one go or if it requires a loop?
Well I can test it.

If this did go into Hesiod I'd need to work out how to resolve the "find_representative_pod5" because
this relies on knowing which channel is active. I guess I could quickly look in the first input pod5 to
find the active channel for the first read. It would work, but is a little hacky. The alternative is to
do all the channel mapping first. This avoids the need for checkpoint rules, but makes the driver logic
more complex. Could be the way?

The PLN:

1) Make and test a standalone Snakefile to do this (with checkpoints).

2) Test it on the latest run and some old ones (with many POD5 files)

Try: /lustre-gseg/promethion/prom_runs/2023/20231107_MIN2_26171SS/26171SS0001L02/20231108_1411_MN32284_AOZ898_35873099/pod5_pass

3) Put it on to WorkflowHub

4) Then worry about Hesiod

Coolio.

--

OK this works but it produces a load of files. This should come as no surprise since there are 4000
pores on a Promethion flowcell. So we need to batch up some channels.

My idea:
I realise I can do this with minimal messing with the Snakefile.
When making the .channels files, convert each channel to a range.
So if we see channel 123 and the batch size is 50 this should be converted
to 100-150. Add this as a third column in the .batches files. Then I think everything
else just works.
